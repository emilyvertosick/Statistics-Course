---
title: "Statistics Course Answers"
output:
  html_document:
    css: custom.css
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(skimr)
library(gtsummary)
library(janitor)
library(epiR)
library(broom)
library(pROC)
library(tidyverse)
Sys.setlocale("LC_CTYPE", "Chinese")
```

```{r loaddata, include = FALSE}
lesson1a <- readRDS(here::here("Data", "Week 1", "lesson1a.rds"))
lesson2a <- readRDS(here::here("Data", "Week 2", "lesson2a.rds"))
lesson2b <- readRDS(here::here("Data", "Week 2", "lesson2b.rds"))
lesson2c <- readRDS(here::here("Data", "Week 2", "lesson2c.rds"))
lesson2d <- readRDS(here::here("Data", "Week 2", "lesson2d.rds"))
lesson2e <- readRDS(here::here("Data", "Week 2", "lesson2e.rds"))
lesson3a <- readRDS(here::here("Data", "Week 3", "lesson3a.rds"))
lesson3b <- readRDS(here::here("Data", "Week 3", "lesson3b.rds"))
lesson3c <- readRDS(here::here("Data", "Week 3", "lesson3c.rds"))
lesson3d <- readRDS(here::here("Data", "Week 3", "lesson3d.rds"))
lesson3e <- readRDS(here::here("Data", "Week 3", "lesson3e.rds"))
lesson3f <- readRDS(here::here("Data", "Week 3", "lesson3f.rds"))
lesson4a <- readRDS(here::here("Data", "Week 4", "lesson4a.rds"))
lesson4b <- readRDS(here::here("Data", "Week 4", "lesson4b.rds"))
lesson4c <- readRDS(here::here("Data", "Week 4", "lesson4c.rds"))
lesson4d <- readRDS(here::here("Data", "Week 4", "lesson4d.rds"))
lesson4e <- readRDS(here::here("Data", "Week 4", "lesson4e.rds"))
lesson5a <- readRDS(here::here("Data", "Week 5", "lesson5a.rds"))
lesson5b <- readRDS(here::here("Data", "Week 5", "lesson5b.rds"))
lesson5c <- readRDS(here::here("Data", "Week 5", "lesson5c.rds"))
lesson5d <- readRDS(here::here("Data", "Week 5", "lesson5d.rds"))
lesson5e <- readRDS(here::here("Data", "Week 5", "lesson5e.rds"))
lesson5f <- readRDS(here::here("Data", "Week 5", "lesson5f.rds"))
lesson5g <- readRDS(here::here("Data", "Week 5", "lesson5g.rds"))
lesson5h <- readRDS(here::here("Data", "Week 5", "lesson5h.rds"))
lesson5i <- readRDS(here::here("Data", "Week 5", "lesson5i.rds"))
lesson6a <- readRDS(here::here("Data", "Week 6", "lesson6a.rds"))
lesson6b <- readRDS(here::here("Data", "Week 6", "lesson6b.rds"))
lesson6c <- readRDS(here::here("Data", "Week 6", "lesson6c.rds"))
lesson6d <- readRDS(here::here("Data", "Week 6", "lesson6d.rds"))
```

# Assignment Answers

## Week 1

**This is data for 386 patients undergoing surgery. What type of data (e.g. continuous, binary, ordinal, nonsense) are each of the variables?**

The data set has 11 variables:

- "id" is clearly a hospital record number. It doesn’t matter what type of variable it is, because you only want to know the type of variable in order to summarize or analyze data, and you’d never want to analyze or summarize patient id.
- "sex" is a binary variable
- "age" is continuous
- "p1", "p2", "p3", "p4" are the pain scores after surgery. They only take integer values between 0 and 6. They would therefore typically regarded as categorical, and because they are clearly ordered (i.e. pain score of 6 is higher than one of 4) these variables can be described are ordinal. However, many statisticians, myself included, would think it perfectly reasonable to treat these variables as continuous. 
- "t": total pain score takes on values between 0 and 24 and can thus be considered continuous
- "x", "y" and "z": should you attempt to summarize a variable if you don’t know what it is? It is obvious for y, this is some kind of hospital location, and is a categorical variable. But what about x and z? As it happens, z looks ordinal but isn’t: it is blood group coded 1=o, 2=a, 3=b, 4=ab.

## Week 2

### lesson2a.rds: Marathon Runners

**This is data from marathon runners: summarize age, sex, race time in minutes (i.e. how long it took them to complete the race) and favorite running shoe.**

Age is continuous and normally distributed (look at a graph or look at the centiles) and so it wouldn’t be unreasonable to describe age in terms of mean and standard deviation (SD) by using `lesson2a %>% skim(age)`: mean (42 years), SD of 10.2. By the way, don’t just copy the readout from R: this would give mean age as 42.43 and implies we are interested in age within a few days. 

Sex is binary: use `lesson2a %>% tabyl(sex)` to get percentages. But note that the person who prepared the data didn’t state how the variable was coded (someone with sex=1 is a woman or a man?). Now what you should do in this situation is ask, but here is another alternative:

```{r week2a}

lesson2a %>%
  group_by(sex) %>%
  skim(rt)

```

So sex==0 are running faster than the sex==1 and it would not be unreasonable to assume that sex==1 means women. 

Race time is continuous and looks pretty normal. Yes the data are skewed by a single outlier (a race time of `r max(lesson2a$rt)` minutes) but the mean and median are pretty similar (`r round(mean(lesson2a$rt, na.rm = TRUE), 0)` v. `r median(lesson2a$rt)` minutes) and the 5th and 95th centile are very close to the values expected by adding or subtracting 1.64 times the SD from the mean (5th centile is `r round(quantile(lesson2a$rt, c(0.05)), 0)`, expected value using means and SD is `r round(mean(lesson2a$rt, na.rm = TRUE) - sd(lesson2a$rt, na.rm = TRUE)*1.64, 0)`; 95th centile actual and expected values are `r round(mean(lesson2a$rt, na.rm = TRUE) + sd(lesson2a$rt, na.rm = TRUE)*1.64, 0)` and `r round(quantile(lesson2a$rt, c(0.95)), 0)`). I would probably feel comfortable reporting a mean and SD if you wanted.

Favorite running shoe: `lesson2a %>% skim(shoe)` gives a mean of `r round(mean(lesson2a$shoe, na.rm = TRUE), 2)` and a median of `r median(lesson2a$shoe, na.rm = TRUE)`. If you reported these, you need therapy: shoe is a categorical variable and you should report the percentage of runners that favor each shoe.

AN ADDITIONAL IMPORTANT POINT: you should give the number of observations and the number of missing observations. n is 98 for all observations except for favorite running shoe, where there are 95 observations and 3 missing observations.

So here is a model answer, suitable for publication (assuming that sex==1 is coded as female.)

<br>

```{r week2b, echo = FALSE, results = 'asis'}

lesson2a <-
  lesson2a %>%
  mutate(
    shoe = lvls_revalue(as_factor(shoe),
                        c("Asics", "New Balance","Nike", "Saucony"))
  )

lesson2a %>%
  select(-tm, -id) %>%
  tbl_summary(
    statistic = list(..continuous.. = "{mean} ({sd}))"),
    label = list(age = "Age", sex = "Women", rt = "Race time in minutes",
                 shoe = "Favorite Running Shoe")
  )

```

<br>

However, look at this.

<br>

```{r week2c, echo = FALSE, results = 'asis'}

lesson2a %>%
  select(-tm, -id) %>%
  tbl_summary(
    label = list(age = "Age", sex = "Women", rt = "Race time in minutes",
                 shoe = "Favorite Running Shoe")
  )

```

<br>

This really isn't much to distinguish between these tables. On the one hand using the mean and SD gives you more information (e.g. what proportion of patients are aged over 70?). On the other hand, I can't see anyone using a table 1 to do these types of calculation and the median and interquartile range give you some more immediately usable information (no multiplying by 1.64!).

### lesson2b.rds: Postoperative Pain

**Summarize average pain after the operation. Imagine you had to draw a graph of "time course of pain after surgery". What numbers would you use for pain at time 1, time 2, time 3, etc.?**

This is data on postoperative pain. You were asked to summarize average pain after the operation. This is continuous, so by looking at the histogram, you can see that the data look skewed. I would be tempted to use the median (`r median(lesson2b$t, na.rm = TRUE)`) and quartiles (`r quantile(lesson2b$t, na.rm = TRUE, c(0.25))`, `r style_sigfig(quantile(lesson2b$t, na.rm = TRUE, c(0.75)))`).

```{r week2d, warning = FALSE}

ggplot(data = lesson2b,
       aes(x = t)) +
  geom_histogram()

```

However, using the mean and standard deviation doesn’t get you far off. For normal data, 50% of the observations are within two-thirds of a standard deviations of the mean. So the interquartile range predicted from the mean and SD would be `r round(mean(lesson2b$t, na.rm = TRUE), 1)` - `r round(sd(lesson2b$t, na.rm = TRUE), 1)`\*0.67 and `r round(mean(lesson2b$t, na.rm = TRUE), 1)` + `r round(sd(lesson2b$t, na.rm = TRUE), 1)`\*0.67 which gives you `r round(mean(lesson2b$t, na.rm = TRUE) - sd(lesson2b$t, na.rm = TRUE)*0.67, 1)` and `r round(mean(lesson2b$t, na.rm = TRUE) + sd(lesson2b$t, na.rm = TRUE)*0.67, 1)`. This is an interesting point: the data can look skewed (and if you are interested, a statistical test can tell you that the data are definitely non-normal) but it doesn’t make much of a difference in practice. 

You were then asked to imagine that you had to draw a graph of "time course of pain after surgery". What numbers would you use for time 1, time 2 etc? The first thing to note is that to draw the graph, which seems like a useful thing to do, you have to treat the data as continuous: you can’t really graph a table very easily. This is another illustration of how something that seems technically incorrect gives you useful approximations. Now that we have continuous data we have to decide whether to use medians or means. As it turns out, normality of the data isn’t even an issue. Here is what happens if you graph the median pain score at each time point.

```{r week2e, echo = FALSE, warning = FALSE}

# Reshape data
lesson2b_long <-
  lesson2b %>%
  gather(v, value, -t) %>%
  separate(v, c("var", "time"), sep = "(?=[[:digit:]])", extra = "merge") %>%
  select(-var) %>%
  mutate(time = as.numeric(time),
         value = as.numeric(value))

# Calculate median and mean at each timepoint
lesson2b_stats <-
  lesson2b_long %>%
  group_by(time) %>%
  summarize(median = median(value, na.rm = TRUE),
            mean = mean(value, na.rm = TRUE))

# Graph median pain
ggplot(data = lesson2b_stats,
       aes(x = time, y = median)) +
  geom_line(col = "darkblue") +
  scale_x_continuous(name = "Time", breaks = c(0:10)) +
  scale_y_continuous(name = "Median Pain Score", breaks = seq(0, 3.5, by = 0.5))

```

<br>

This makes it seem that postoperative pain doesn’t change for four time points, then drops dramatically. 

A graph using means seems more illustrative of what is really going on.

```{r week2f, echo = FALSE, warning = FALSE}

# Graph mean pain
ggplot(data = lesson2b_stats,
       aes(x = time, y = mean)) +
  geom_line(col = "darkblue") +
  scale_x_continuous(name = "Time", breaks = c(0:10)) +
  scale_y_continuous(name = "Median Pain Score", breaks = seq(0, 3.5, by = 0.5))

```

### lesson2c.rds: Radical Prostatectomy Cohort

**This is data on 242 patients undergoing radical prostatectomy. Summarize age, stage, grade and PSA.**

One of the first things to look for here is missing data. If you type `lesson2c %>% skim()`, you’ll see that there is no missing data for age, 9 missing observations for PSA and 41 for grade. You will notice that the "stage" variable is not included here, because it is a character variable, not a numeric variable.

If you type `lesson2c %>% tabyl(stage)`, you'll see that all patients have a stage assigned (no "NA" values).

A second issue is how to characterize the categorical variables. There are 8 different stages represented, many with very small numbers (like 2 for T4 and T1B, 4 for T3). It generally isn’t very helpful to slice and dice data into very small categories, so I would consider grouping. For instance, we could group the T1s and T3 and 4 to get something like:

<br>

```{r week2g, echo = FALSE, warning = FALSE, result = 'asis'}

lesson2c <-
  lesson2c %>%
  mutate(
    stage_category =
      case_when(
        stage %in% c("T1A", "T1B", "T1C") ~ "T1",
        stage %in% c("T3", "T4") ~ "T3/4",
        TRUE ~ stage
      )
  )

lesson2c %>%
  select(stage_category) %>%
  tbl_summary(label = list(stage_category = "Stage"))

```

<br>

To get the table, I created a new variable as follows:

```{r week2h, echo = TRUE, eval = FALSE}

lesson2c <-
  lesson2c %>%
  mutate(
    stage_category =
      case_when(
        stage %in% c("T1A", "T1B", "T1C") ~ "T1",
        stage %in% c("T3", "T4") ~ "T3/4",
        TRUE ~ stage
      )
  )

```

The `case_when` function is similar to the `if_else` function, but it allows for more than the 2 options of TRUE/FALSE. In this case, `TRUE` indicates all values that were not included in the prior categories.

There is a similar issue for grade, with few patients having grades 4, 5, 8 or 9. Now a key point is that what you decide to do in situations like this will often need to take into account your medical understanding. It might seems sensible to categorize grade as ≤6 or ≥7, or perhaps 4/5, 6, 7, 8/9. But as it turns out in prostate cancer, pathologists can’t reliably grade a cancer as 4 or 5 and these cancers should really be grouped with grade 6. Grade 8, on the other hand, signifies very aggressive disease and really needs to be reported separately even if there are only a few patients with grade 8. So grade would probably be summarized as per the following table:

<br>

```{r week2i, echo = FALSE, warning = FALSE, results = 'asis'}

lesson2c %>%
  mutate(
    grade_category =
      case_when(
        grade <= 6 ~ 6,
        grade >= 8 ~ 8,
        TRUE ~ grade
      )
  ) %>%
  select(stage_category, grade_category, age, psa) %>%
  tbl_summary(label = list(stage_category = "Stage",
                           grade_category = "Grade",
                           age = "Age",
                           psa = "PSA"))

```

<br>

### lesson2d.rds: Cost of a Minor Medical Procedure

**Summarize cost.**

I asked you to summarize the cost. You can see from a graph that the data are not normally distributed. If we were to follow the rule book slavishly, we would report a median and interquartile range. But what use is a median for cost data? We want to know "average" cost so that we can predict, for example, how much we should budget for next year. This requires a mean.

### lesson2e.rds: Cancer pain

**Summarize total cancer pain in one month in a group of chronic cancer patients.**

The data are grossly non-normal and you could use the median pain score with interquartile range. 

As for summarizing the number of days in pain, almost everyone has 31 days of pain. So I would give proportions of patients who had pain for 31 days and then the number who, say, had pain at least 3 out of every 4 days and 2 out of 4 days. There are two ways of doing this. You could use `lesson2e %>% tabyl(f)` and then combine some of the results. Or, and this is a little more complicated, you could create a new variable.


```{r week2j}

lesson2e <-
  lesson2e %>%
  mutate(
    days =
      case_when(
        f <= 15 ~ 0,
        f > 15 & f < 23 ~ 1,
        f > 23 & f < 31 ~ 2,
        f == 31 ~ 3
      )
  )

```

In other words, create a new variable called "days", and set it to 0 for anyone with 15 or less days of pain. Call anyone who has pain more than half the time (i.e. more than 15 days) a 1. Call anyone who has pain more than three quarter of the time (i.e. more than 23 days) a 2. Call anyone who has pain all the time a 3.

```{r week2k}

lesson2e %>%
  tabyl(days)

```

So you could report that, of the `r sum(!is.na(lesson2e$days))` patients with data on number of days with pain, 51% were in daily pain, `r round(tabyl(lesson2e$days)$valid_percent[1]*100, 0)`% of patients had pain on less than half of days, `r round(tabyl(lesson2e$days)$valid_percent[2]*100, 0)`% of patients had pain on more than half but less than 75% of days, `r round(tabyl(lesson2e$days)$valid_percent[3]*100, 0)`% of patients had pain on more than 75% of days, but not every day.

<br>

_For keen students only!_

You could also try a log transformation of the pain scores. Create a new variable using `log(pain)`.

```{r week2l}

lesson2e <-
  lesson2e %>%
  mutate(log = log(pain))

```

Analyze this variable and you’ll see it appears normally distributed.

The mean of log is `r round(mean(lesson2e$log, na.rm = TRUE), 2)`. You can transform the log back by calculating e^`r round(mean(lesson2e$log, na.rm = TRUE), 2)`^ (the function is ``exp(`r round(mean(lesson2e$log, na.rm = TRUE), 2)`)``) to get `r round(exp(mean(lesson2e$log, na.rm = TRUE)), 0)`, close to the median. Backtransforming the standard deviation is more complicated and can’t really be done directly. What you need to do is make any calculations you need on the log transformed scale and then backtransform the results. Imagine you wanted a 95% confidence interval. A 95% confidence interval is the mean ± 1.96 standard deviations. The standard deviation of the log data is close to one. So the confidence interval is `r round(mean(lesson2e$log, na.rm = TRUE) - 1.96*sd(lesson2e$log, na.rm = TRUE), 2)` to `r round(mean(lesson2e$log, na.rm = TRUE) + 1.96*sd(lesson2e$log, na.rm = TRUE), 2)`. Transforming this ``exp(`r round(mean(lesson2e$log, na.rm = TRUE) - 1.96*sd(lesson2e$log, na.rm = TRUE), 2)`)`` and ``exp(`r round(mean(lesson2e$log, na.rm = TRUE) + 1.96*sd(lesson2e$log, na.rm = TRUE), 2)`)`` gives a confidence interval on the original scale of `r round(exp(mean(lesson2e$log, na.rm = TRUE) - 1.96*sd(lesson2e$log, na.rm = TRUE)), 2)` and `r round(exp(mean(lesson2e$log, na.rm = TRUE) + 1.96*sd(lesson2e$log, na.rm = TRUE)), 2)`. This is a reasonable approximation to the 5th (`r round(quantile(lesson2e$pain, na.rm = TRUE, c(0.05)), 0)`) and 95th centile (`r round(quantile(lesson2e$pain, na.rm = TRUE, c(0.95)), 0)`).

## Week 3

### lesson3a.rds

**These are data from over 1000 patients undergoing chemotherapy reporting a nausea and vomiting score from 0 to 10.  Does previous chemotherapy increase nausea scores? What about sex?**

```{r week2m, include = FALSE}

n_fulldata <- 
  lesson3a %>%
  filter(!is.na(pc) & !is.na(nv)) %>%
  summarize(sum = nrow(.))

meanchemo <-
  lesson3a %>%
  filter(pc == 1 & !is.na(nv)) %>%
  summarize(n = nrow(.), mean = mean(nv, na.rm = TRUE), sd = sd(nv, na.rm = TRUE))

meannochemo <-
  lesson3a %>%
  filter(pc == 0 & !is.na(nv)) %>%
  summarize(n = nrow(.), mean = mean(nv, na.rm = TRUE), sd = sd(nv, na.rm = TRUE))

meanpct <-
  ((meanchemo %>% pull(mean) - meannochemo %>% pull(mean)) /
     meannochemo %>% pull(mean))*100

chemo_test <-
  t.test(nv ~ pc, data = lesson3a, var.equal = TRUE, paired = FALSE)

meanwomen <-
  lesson3a %>%
  filter(sex == 1 & !is.na(nv)) %>%
  summarize(n = nrow(.), mean = mean(nv, na.rm = TRUE), sd = sd(nv, na.rm = TRUE))

sextotal <-
  lesson3a %>%
  group_by(sex) %>%
  summarize(n = n())

sexmiss <-
  lesson3a %>%
  filter(is.na(nv)) %>%
  group_by(sex) %>%
  summarize(n = n())

meanmen <-
  lesson3a %>%
  filter(sex == 0 & !is.na(nv)) %>%
  summarize(n = nrow(.), mean = mean(nv, na.rm = TRUE), sd = sd(nv, na.rm = TRUE))

women_test <-
  t.test(nv ~ sex, data = lesson3a, var.equal = TRUE, paired = FALSE)

chi_test <-
  chisq.test(table(lesson3a$sex, !is.na(lesson3a$nv)), correct = FALSE)

```

Hands up, who typed in `t.test(nv ~ pc, data = lesson3a, var.equal = TRUE)` without looking at the data? There are lots of missing data. There is a question as to whether you would actually analyze these data at all: could data be missing because patients were too ill to complete questionnaires?  Could there have been bias in ascertaining who had prior chemotherapy?  If you do decide to analyze, a t-test would be appropriate (`t.test(nv ~ pc, data = lesson3a, var.equal = TRUE)` and `t.test(nv ~ sex, data = lesson3a, var.equal = TRUE)`). A model answer might be:

<div class="quote-container">

> Details of prior chemotherapy were available for `r pull(n_fulldata)` of the `r nrow(lesson3a)` patients with nausea scores. Mean nausea scores were approximately `r round(meanpct, -1)`% higher in the `r meanchemo %>% pull(n)` patients with prior experience of chemotherapy (`r round(meanchemo %>% pull(mean), 1)`; SD `r round(meanchemo %>% pull(sd), 2)`) than in the `r meannochemo %>% pull(n)` chemotherapy-naive patients  (`r round(meannochemo %>% pull(mean), 1)`; SD `r round(meannochemo %>% pull(sd), 2)`). The difference between groups was small (`r round(meanchemo %>% pull(mean) - meannochemo %>% pull(mean), 1)`, 95% C.I. `r round(abs(chemo_test$conf.int[[2]]), 1)`, `r round(abs(chemo_test$conf.int[[1]]), 1)` but highly statistically significant (p`r style_pvalue(chemo_test$p.value)` by ttest), suggesting that prior chemotherapy increases nausea scores. Nausea scores were slightly lower in women (n=`r meanwomen %>% pull(n)`; mean `r round(meanwomen %>% pull(mean), 1)`; SD `r round(meanwomen %>% pull(sd), 2)`) than men (n=`r meanmen %>% pull(n)`; mean `r round(meanmen %>% pull(mean), 1)`; SD `r round(meanmen %>% pull(sd), 2)`) but there were no statistically significant differences between the sexes (difference between means `r round(meanmen %>% pull(mean) - meanwomen %>% pull(mean), 1)`, 95% C.I. `r round(women_test$conf.int[[1]], 1)`, `r round(women_test$conf.int[[2]], 1)`); p=`r style_pvalue(women_test$p.value)` by ttest). However, far more women (`r sexmiss$n[2]` / `r sextotal$n[2]`, `r round(sexmiss$n[2]/(sexmiss$n[1]+sexmiss$n[2])*100, 0)`%) than men (`r sexmiss$n[1]` / `r sextotal$n[1]`, `r round(sexmiss$n[1]/(sexmiss$n[1]+sexmiss$n[2])*100, 0)`%) failed provide a nausea score (p=`r style_pvalue(chi_test$p.value)` by chi squared), perhaps suggesting bias in reporting.

</div>

Now, I'll explain what I did. First, I did the two ttests:

```{r week3a}

t.test(nv ~ pc, data = lesson3a, var.equal = TRUE)
t.test(nv ~ sex, data = lesson3a, var.equal = TRUE)

```

Then I created a new variable for missing data on nausea and vomiting:

```{r week3b}

lesson3a <-
  lesson3a %>%
  mutate(
    missing =
      if_else(is.na(nv), 1, 0)
  )

```

And then the table:

```{r week3c, warning = FALSE}

tabyl(lesson3a, missing, sex) %>%
  adorn_title()
# adorn_title() is used to show the variable names on the table

```

If you got that far: wow! (also, forget the course, you don’t need it). If you didn’t get that far, don’t feel bad about it, but try to get a handle on the thought process.

One other issue: the p value for the first ttest (previous chemotherapy) is given as "p-value = 4.709e-06". While this number is very close to 0, we cannot round to 0 - there is no such thing as a p value of 0 for any hypothesis worth testing (there is a small but finite chance of every possible result, even throwing 100,000 tails in a row on an unbiased coin.) So do not report "p=0.0000"! You can round p values for reporting - for example, "4.709e-06" can be rounded to "<0.0005".

<br>

_For more advanced students:_

The other thing you can do is to find out the precise p value from the t value (which is given above the p value: -4.6275). The function you need is `pt(t value, degrees of freedom)`. This will give you the p value for a one-sided test, so you must multiply by 2 to get the two-sided test p value.

```{r week3d}

pt(-4.6275, 510)*2

```

This p-value can also be read as `r round(pt(-4.6275, 510)*2*10^6, 1)` x 10^-6^. An interesting question though: is it important whether p is <0.0005 or `r round(pt(-4.6275, 510)*2*10^6, 1)` x 10^-6^?

What is "degrees of freedom" and how come it is `r t.test(nv ~ pc, data = lesson3a, var.equal = TRUE)$parameter`? Think about it this way: You have a line of ten people outside your door. You know the mean age of this group. Each person comes in one-by-one, you try to guess their age and they tell you how old they actually are. As each person comes in, your guesses will get better and better (for example, if the first three people have ages less than the mean, you will guess the age of the fourth person as something above the mean). However, only when you have the ages of the first nine people will you be able to guess for sure the age of the next (and last) person. In other words, the ages of the first nine people are "free", the age of the last person is "constrained". So "degrees of freedom" is the sample size minus one. Little catch though: for an unpaired test (such a straight drug v. placebo trial), you have two different groups and two different means. You would therefore be able to predict the scores of two observations (the last patient in each group). Degrees of freedom in an unpaired test is therefore the total sample size minus two (or, put another way, the sample size in group a minus one plus the sample size in group b minus one.)

### lesson3b.rds

**Patients with wrist osteoarthritis are randomized to a new drug or placebo. Pain is measured before and after treatment. Is the drug effective?**

The data look relatively normal and the most obvious thing to do would be to do:

```{r week3e}

t.test(p ~ g, data = lesson3b, var.equal = TRUE)

```

This gives a p value of `r style_pvalue(t.test(p ~ g, data = lesson3b, var.equal = TRUE)$p.value)` and you might conclude that there is a "trend" towards statistical significance suggesting that the drug works. However, the ttest assumes that the data are independent. In the present case, this assumption does not hold: each patient contributes data from two wrists, and the pain scores from each wrist are correlated. There are several ways around the problem. The most obvious is to say: "These data are not independent, I am not going to analyze them. Call in a statistician."

However, if you are really keen, you could try the following:

You could analyze the data from each wrist separately by creating new variables as shown below (lines with # are comments).

```{r week3f}

lesson3b <-
  lesson3b %>%
  # Create a new variable called "pright" equivalent to "p", the pain score for the right wrist (site 2) only
  mutate(
    pright = case_when(site == 2 ~ p)
    # By default, any observations that do not fall into the specified conditions in the "case_when" statement will be set to missing (NA)
  ) %>%
  # Create a new variable called "pleft" for pain scores for the left wrist
  mutate(
    pleft = case_when(site == 1 ~ p)
  )

# ttest for pain in right and left wrists separately
t.test(pright ~ g, data = lesson3b, var.equal = TRUE)
t.test(pleft ~ g, data = lesson3b, var.equal = TRUE)

```

### lesson3c.rds

**Some postoperative pain data again. Is pain on day 2 different than pain on day 1? Is pain on day 3 different from pain on day 2?**

The most obvious issue here is that you are measuring the same patients on two occasions, so you are going to want a paired test. Should you do a paired ttest? Many statisticians would prefer a non-parametric method given that the data take only five different values and that the number of observations is small. The following command gives you the "Wilcoxon signed rank test":

```{r week3g}

wilcox.test(lesson3c$t1, lesson3c$t2, correct = FALSE, exact = FALSE, paired = TRUE)

```

(By the way: I know this because that is what it says on the read out: if you had asked me yesterday, I doubt I would have remembered the name of a non-parametric paired test, another reason to think in concepts rather than remembering statistical techniques). The p value you get is p=`r style_pvalue(wilcox.test(lesson3c$t1, lesson3c$t2, correct = FALSE, exact = FALSE, paired = TRUE)$p.value)`. We cannot conclude that pain scores are different. But is that all we want to say? The number of patients is small, maybe we failed to spot a difference. So let’s try a ttest:

```{r week3h}

t.test(lesson3c$t1, lesson3c$t2, var.equal = TRUE, paired = TRUE)

```

The p value you get (p=`r style_pvalue(t.test(lesson3c$t1, lesson3c$t2, var.equal = TRUE, paired = TRUE)$p.value)`) is very similar to the non-parametric method and you get a confidence interval for the difference between means of `r style_sigfig(t.test(lesson3c$t1, lesson3c$t2, var.equal = TRUE, paired = TRUE)$conf.int[[1]])`, `r style_sigfig(t.test(lesson3c$t1, lesson3c$t2, var.equal = TRUE, paired = TRUE)$conf.int[[2]])`. What I would conclude from this is that pain is unlikely to be worse on day 2 than day 1 but any decrease in pain is probably not important. 

What about normality of the data? Doesn’t that figure into whether you assess by ttest or non-parametric? Well if you type look at the distribution of t1 and t2, you find that neither are normally distributed. But a paired ttest does not depend on the assumption that each set of data in a pair is distributed but that the differences between pairs are normally distributed. So you would have to create a new variable and look at that:

```{r week3i}

lesson3c <-
  lesson3c %>%
  mutate(delta12 = t2 - t1)

skim(lesson3c$delta12)

```

As it turns out, the distribution of differences between pain scores are normally distributed even though pain at time 2 does not have a normal distribution. In fact, this is almost always the case. 

Now let's compare t2 and t3. Again, no significant difference. Does this mean that pain does not decrease after an operation? Of course, we know that pain does indeed get better. This illustrates two points: first, don't ask questions you know the answer to; second, asking lots of questions (is pain on day 2 better than on day 1?; is pain on day 3 better on day 2?) is not as good as just asking one question: does pain decrease over time? We'll discuss how to answer this question later on in the course.

### lesson3d.rds

**This is a single-arm, uncontrolled study of acupuncture for patients with neck pain. Does acupuncture increase range of motion?  Just as many men as women get neck pain. Can we say anything about the sample chosen for this trial with respect to gender? The mean age of patients with neck pain is 58.2 years. Is the trial population representative with respect to age?**

This is before and after data, so again you’ll want a paired test. The data are continuous, so a paired ttest is an option (if you are worried about normality, do both the parametric and non-parametric test and compare the results.)

There are two ways of doing the ttest:

1) Test whether the baseline scores are different from the post-treatment scores:

```{r week3j}

t.test(lesson3d$b, lesson3d$a, var.equal = TRUE, paired = TRUE)

```

2) Test whether the difference between scores is different from zero

```{r week3k}

t.test(lesson3d$d, mu = 0)

```

Both methods give you an identical p value of `r style_pvalue(t.test(lesson3d$d, mu = 0)$p.value)`. But note, I didn’t ask for the p value. I asked "Does acupuncture increase range of motion?" Given that this is an uncontrolled trial, it may be that range of motion has improved naturally over time. So in fact you can’t answer the question about whether acupuncture increases range of motion from these data!

BTW: for those who are interested, acupuncture has been shown to improve neck pain and range of motion in a controlled trial, see: Irnich D, Behrens N, Gleditsch JM, Stör W, Schrieber MA,  Schöps P, Vickers AJ, Beyer A. Immediate effects of dry needling and acupuncture at distant points in chronic neck pain: results of a randomized, double-blind, sham-controlled crossover trial. Pain 2002;99(1-2):83)

```{r week3l}

binom.test(sum(lesson3d$sex), nrow(lesson3d), p = 0.5)

```

Only about a quarter of the patients are women, and the binomial test gives a p value of `r style_pvalue(binom.test(sum(lesson3d$sex), nrow(lesson3d), p = 0.5)$p.value)`, suggesting that men are over-represented in this trial. But is this important?

```{r week3m}

t.test(lesson3d$age, mu = 58.2)

```

Similarly, this ttest for age gives a p value of `r style_pvalue(t.test(lesson3d$age, mu = 58.2)$p.value)`, but I doubt anyone would call a mean age of 52 much different from a mean age of 58, so you’d probably want to say that, ok, the trial patients are younger than we might expect, but not by much.

### lesson3e.rds

**These data are length of stay from two different hospitals. One of the hospitals uses a new chemotherapy regime that is said to reduce adverse events and therefore length of stay. Does the new regime decrease length of stay?**

Well, before running off and doing ttests, let’s have a look at the data: 

```{r week3n, warning = FALSE}

lesson3e %>%
  group_by(hospital) %>%
  skim()

```

This shows us the mean, median etc. for length of stay by hospital. The two sets of data look very similar. Regardless of whether we do a ttest or non-parametric test, we do not see a difference between groups, p is `r t.test(los ~ hospital, data = lesson3e, var.equal = TRUE, paired = FALSE)$p.value` or `r style_pvalue(wilcox.test(los ~ hospital, data = lesson3e, correct = FALSE, exact = FALSE, paired = FALSE)$p.value)`. Interesting question: should you report a 95% confidence interval for the difference between groups? This shows, for example, that the new regimen could reduce length of stay by up to six days, surely important in cost terms. On the other hand, this is not a randomized trial. There might be all sorts of differences between the hospitals other than the different chemotherapy regime, such as the mix of patients, discharge policies etc. So you would have to be careful in stating that the chemotherapy regime "could reduce hospital stay by as much as six days" or some such. 

For advanced students only:

The data are non-normally distributed so you could try a log transform: 

```{r week3o}

lesson3e <-
  lesson3e %>%
  mutate(loglos = log(los))

```

The distribution of this new variable for both groups combined is normal, therefore data in each group will be normal. Try a ttest:

```{r week3p}

t.test(loglos ~ hospital, data = lesson3e, var.equal = TRUE, paired = FALSE)

```

```{r week3q, include = FALSE}

log_test <- t.test(loglos ~ hospital, data = lesson3e, var.equal = TRUE, paired = FALSE)

log_diff <- log_test$estimate[[1]] - log_test$estimate[[2]]

```

The p value is similar to the untransformed analyses. Now, look at the difference between means `r style_sigfig(log_test$estimate[[1]] - log_test$estimate[[2]])` and confidence interval (`r style_sigfig(log_test$conf.int[[1]])`, `r style_sigfig(log_test$conf.int[[2]])`).

To backtransform these values, you have to remember that addition on a log scale is the same as multiplication on an untransformed scale. Backtransform `r style_sigfig(log_diff)` using ``exp(`r style_sigfig(log_diff)`)`` and you get `r style_sigfig(exp(log_diff))`. In other words, length of stay in hospital a is `style_sigfig(exp(log_diff)*100)`% (or `r style_sigfig(100 - exp(log_diff)*100)`% less) compared to hospital b. Do the same things with the upper and lower bounds of the confidence interval and you might conclude that the difference in length of stay is between `r style_sigfig(100 - exp(log_test$conf.int[[1]])*100)`% less in hospital a to `r style_sigfig(abs(100 - exp(log_test$conf.int[[2]])*100))`% less in hospital b. If you wanted to convert these numbers to actual days, just multiply by the mean hospital stay in group b by `r style_sigfig(exp(log_test$estimate[[1]] - log_test$estimate[[2]]))`, `style_sigfig(exp(log_test$conf.int[[1]]))` and `r style_sigfig(exp(log_test$conf.int[[2]]), digits = 3)`.

### lesson3f.rds

**These data are from a randomized trial on the effects of physiotherapy treatment on function in pediatric patients recovering from cancer surgery. Function is measured on a 100 point scale. Does physiotherapy help improve function?**

You will want to do an unpaired ttest or non-parametric test on these data. 

```{r week3s}

t.test(delta ~ physio, data = lesson3f, var.equal = TRUE, paired = FALSE)

```

```{r week3t}

physio_test <- t.test(delta ~ physio, data = lesson3f, var.equal = TRUE, paired = FALSE)

meansd1 <- lesson3f %>% filter(physio == 1) %>% summarize(mean = mean(delta, na.rm = TRUE), sd = sd(delta, na.rm = TRUE))

meansd2 <- lesson3f %>% filter(physio == 0) %>% summarize(mean = mean(delta, na.rm = TRUE), sd = sd(delta, na.rm = TRUE))

```

This gives a p value of `r style_pvalue(physio_test$p.value)` (incidentally, don't say "p > 0.05" or "p=`r style_sigfig(physio_test$p.value, digits = 4)`", which are under- and over-precise, respectively). Given a p value greater than 5%, we would say that we found no evidence that the treatment was effective. But is this really the case? Look at the read out from the ttest again, particularly the means for each group and the difference. The improvements in the treatment group was about 50% greater than in controls. The 95% confidence interval for the difference includes a possible `r style_sigfig(abs(physio_test$conf.int[[1]]))` point improvement on physiotherapy, more than twice that of controls. So physiotherapy might well be a clinically important treatment in this population, even if we failed to prove it was better than control in this trial. Whatever your conclusion, don’t just report the p value: report the means and standard deviations in each group separately plus the difference between means and a 95% confidence interval. If I was writing this up for a journal, I might say:

<div class="quote-container">

>Mean improvement in function was greater in the `r sum(lesson3f$physio)` physiotherapy group patients (`r style_sigfig(meansd1 %>% pull(mean), digits = 3)` points, SD `r style_sigfig(meansd1 %>% pull(sd), digits = 3)`) than in the `r nrow(lesson3f) - sum(lesson3f$physio)` controls (`r style_sigfig(meansd2 %>% pull(mean), digits = 3)`, SD `r style_sigfig(meansd2 %>% pull(sd), digits = 3)`). Although by ttest the difference between groups (`r style_sigfig(abs(physio_test$estimate[[1]] - physio_test$estimate[[2]]))`) was not statistically significant (`r style_pvalue(physio_test$p.value)`), the 95% confidence interval (`r style_sigfig(physio_test$conf.int[[2]]*-1)`, `r style_sigfig(physio_test$conf.int[[1]]*-2, digits = 3)`) includes clinically relevant effects. This suggests that the trial may have been underpowered to detect meaningful differences between groups.

</div>

An alternative conclusion, which I think I actually prefer, would be to focus on the difference between groups of `r style_sigfig(abs(physio_test$estimate[[1]] - physio_test$estimate[[2]]))`, deciding whether this is clinically significant.

## Week 4

### lesson4a.rds

**This is a data set on fifteen patients recording whether they had problematic nausea or vomiting after chemotherapy (defined as grade 2 or higher for either nausea or vomiting) and whether they reported being prone to travel sickness. Does travel sickness predict chemotherapy nausea and vomiting?**

To have a quick look at the data, create a two-by-two table:

```{r week4a, warning = FALSE}

lesson4a %>%
  tabyl(nv, cs)

lesson4a %>%
  tabyl(nv, cs) %>%
  adorn_percentages("col")

```

```{r week4b, warning = FALSE, include = FALSE}

nv_pct <-
  lesson4a %>%
  tabyl(nv, cs) %>%
  adorn_percentages("col")

```

This shows, for example, `r round(nv_pct[2, 3]*100, 0)`% of those who get car sick reported problematic nausea compared to only `r round(nv_pct[2, 2]*100, 0)`% of those who aren't prone to car sickness. To find out whether this is statistically significant (it certainly seems clinically significant), you can use the `chisq.test` or the `fisher.test` function.

```{r week4c, warning = FALSE}

chisq.test(lesson4a %>% tabyl(nv, cs), correct = FALSE)

fisher.test(lesson4a %>% tabyl(nv, cs))

```

`chisq.test` gives the p value from a chi-squared test, `fisher.test` is a special form of this test when any of the numbers in the table are small, say 10 or below). The p-value is about 0.6. You now have 3 options:

1. Declare the result non-statistically significant, and conclude that you have failed to reject the null hypothesis of no difference between groups.
2.	Say that, though differences between groups were not statistically significant, sample size was too small.
3.	Say that differences between groups were not statistically significant, and that the study was too small, but try to quantify a plausible range of possible differences between groups.

For point 3, you need the confidence interval. You get this by using the `epi.2by2` command.

```{r week4d, warning = FALSE}

epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4a, cs, nv), desc(cs))), 1:2)))

```

```{r week4e, include = FALSE, warning = FALSE}

tbl_cs <-
  epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4a, cs, nv), desc(cs))), 1:2)))

exact_cs <-
  fisher.test(lesson4a %>% tabyl(nv, cs))

```

This gives a "risk ratio" (same as relative risk) of `r style_ratio(tbl_cs$res$RR.crude.wald[[1]])` with a 95% CI `r style_ratio(tbl_cs$res$RR.crude.wald[[2]])`, `r style_ratio(tbl_cs$res$RR.crude.wald[[3]])`. A model answer might be:

<div class="quote-container">

>`r str_to_title(xfun::numbers_to_words(as.numeric(tbl_cs$tab[1, 1])))` of the `r xfun::numbers_to_words(as.numeric(tbl_cs$tab[1, 3]))` patients (`r style_sigfig(as.numeric(tbl_cs$tab[1, 4]))`%) who reported prior car sickness had grade 2 or higher nausea and vomiting compared to `r xfun::numbers_to_words(as.numeric(tbl_cs$tab[2, 1]))` of the `r xfun::numbers_to_words(as.numeric(tbl_cs$tab[2, 3]))` (`r style_sigfig(as.numeric(tbl_cs$tab[2, 4]))`%) who reported no prior car sickness. Though differences between groups were not statistically significant (p=`r style_pvalue(exact_cs$p.value)` by Fisher's exact test), the sample size was small and the 95% confidence interval for the difference between groups includes differences of clinical relevance (relative risk `r style_ratio(tbl_cs$res$RR.crude.wald[[1]])`; 95% CI `r style_ratio(tbl_cs$res$RR.crude.wald[[2]])`, `r style_ratio(tbl_cs$res$RR.crude.wald[[3]])`). It may be, for example, that problematic nausea and vomiting is up to six times more common in patients reporting prior car sickness than in those who do not.

</div>

### lesson4b.rds

**An epidemiological study of meat consumption and hypertension. Meat consumption was defined as low, medium or high depending on whether subjects ate less than 3, 3 to 7 or 7 + meals with meat in per week. Does meat consumption lead to hypertension?**

There are three levels of meat consumption. The outcome (hypertension) is binary either 1 or 0. So we will have a 3 by 2 table. Let’s start with a quick look:

```{r week4f, warning = FALSE}

lesson4b %>%
  tabyl(meat, hbp) %>%
  adorn_totals() %>%
  adorn_title()

lesson4b %>%
  tabyl(meat, hbp) %>%
  adorn_percentages("row") %>%
  adorn_pct_formatting(digits = 0) %>%
  adorn_title()

# adorn_pct_formatting(digits = 0) rounds percentages to the nearest integer to make the table easier to read

```

```{r week4g, include = FALSE, warning = FALSE}

tbl_meatrow <-
  lesson4b %>%
  tabyl(meat, hbp) %>%
  adorn_percentages("row") %>%
  adorn_pct_formatting(digits = 0)

tbl_meatcol <-
  lesson4b %>%
  tabyl(meat, hbp) %>%
  adorn_percentages("col") %>%
  adorn_pct_formatting(digits = 0)

```

These tables show the number and percentages of patients with hypertension in each category of meat eating. Rates of hypertension are `r tbl_meatrow[1, 3]`, `r tbl_meatrow[2, 3]` and `r tbl_meatrow[3, 3]` in the low, medium and high meat consumption groups, respectively. You also could have created this table:

```{r week4h, warning = FALSE}

lesson4b %>%
  tabyl(meat, hbp) %>%
  adorn_percentages("col") %>%
  adorn_pct_formatting(digits = 0) %>%
  adorn_title()

```

This would tell you, for example, that of the patients with hypertension, `r tbl_meatcol[3, 3]` ate a lot of meat and only `r tbl_meatcol[1, 3]` ate a little. I find this a less helpful statistic than presenting the data in terms of rates of hypertension for level of meat consumption. One other thing to notice is that the rates of hypertension are high, suggesting this is a selected population.

To assess statistical significance, use the `chisq.test` function with the hypertension by meat consumption table as the input.

```{r week4i, warning = FALSE}

chisq.test(lesson4b %>% tabyl(meat, hbp), correct = FALSE)

```

The p value is p = `r style_pvalue(chisq.test(lesson4b %>% tabyl(meat, hbp))$p.value)`. How to interpret this? The formal statistical interpretation is that we can reject the null hypothesis that rates of hypertension are the same in each group. To demonstrate this, try renumbering the labels for the variable meat:

```{r week4j, warning = FALSE}

lesson4b_changed <-
  lesson4b %>%
  mutate(
    meat =
      case_when(
        meat == 3 ~ 0.98573,
        meat == 2 ~ 60,
        meat == 1 ~ 643
      )
  )

chisq.test(lesson4b_changed %>% tabyl(meat, hbp), correct = FALSE)

```

You get exactly the same result. This demonstrates that: a) the numbers you use for meat consumption act only as labels; b) the order is unimportant: you get the same p value if you arrange the columns "low med hi" as "hi low med".

So it might be useful to do what are called "pairwise" comparisons: what difference is there in the rate of hypertension between low and medium meat eaters? What about high and low? Though there are three possible comparisons (hi v. lo; hi v. med; med v. lo), it is more usual to chose either the highest or lowest category and compare everything to that. The way to do these analyses is to create new variables. The code is given below (the #s are comments that are ignored by R).

```{r week4k, warning = FALSE}

# Create a new variable "c" which is "1" if meat consumption is high and "0" if it is medium
# "c" is missing for low meat consumption: these patients are left out of the analysis

lesson4b <-
  lesson4b %>%
  mutate(
    c =
      case_when(
        meat == 2 ~ 0,
        meat == 3 ~ 1
      )
  )

# Since there are now "NA" values, we need to use the "show_na" option to exclude them

epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4b, c, hbp, show_na = FALSE), desc(c))), 1:2)))

```

```{r week4l, warning = FALSE, include = FALSE}

# Overall chi-squared p value
meat_chi <-
  chisq.test(lesson4b %>% tabyl(meat, hbp), correct = FALSE)

# High vs medium
tbl_himed <-
  epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4b, c, hbp, show_na = FALSE), desc(c))), 1:2)))

```

```{r week4m, warning = FALSE}

# Compare high vs low consumption
# "c" is now "1" if meat consumption is high and "0" if it is low
# "c" is missing for medium meat consumption: these patients are left out of the analysis

lesson4b <-
  lesson4b %>%
  mutate(
    c =
      case_when(
        meat == 1 ~ 0,
        meat == 3 ~ 1
      )
  )

epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4b, c, hbp, show_na = FALSE), desc(c))), 1:2)))

```

```{r week4n, warning = FALSE, include = FALSE}

tbl_hilo <- 
  epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4b, c, hbp, show_na = FALSE), desc(c))), 1:2)))

# Compare low and medium for results\
lesson4b <-
  lesson4b %>%
  mutate(
    c =
      case_when(
        meat == 1 ~ 0,
        meat == 2 ~ 1
      )
  )

tbl_medlo <- 
  epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4b, c, hbp, show_na = FALSE), desc(c))), 1:2)))

```

What these commands do is to create a new variable (c) that is used instead of the variable "meat" in analyses. This is set to 1 or 0 or missing depending on the pairwise comparison that you want to do. A model answer might be as follows: 

<div class="quote-container">

>The results of the study are shown in the table. Differences in rates of hypertension between categories of meat consumption are statistically significant (p=`r style_pvalue(meat_chi$p.value)`). Rates of hypertension rise with increasing meat consumption: patients with the highest meat consumption had a risk of hypertension `r style_sigfig(tbl_himed$res$RR.crude.wald[[1]])` times greater than those with intermediate meat consumption (95% CI `r style_sigfig(tbl_himed$res$RR.crude.wald[[2]])`, `r style_sigfig(tbl_himed$res$RR.crude.wald[[3]])`, p=`r style_pvalue(tbl_himed$res$chisq.crude[[3]])`) and `r style_sigfig(tbl_hilo$res$RR.crude.wald[[1]])` times greater than those with the lowest levels of meat consumption (95% CI `r style_sigfig(tbl_hilo$res$RR.crude.wald[[2]])`, `r style_sigfig(tbl_hilo$res$RR.crude.wald[[3]])`, p=`r style_pvalue(tbl_hilo$res$chisq.crude[[3]])`). Rates of hypertension were similar between medium and low meat consumption (relative risk `r style_sigfig(tbl_medlo$res$RR.crude.wald[[1]])`; 95% CI `r style_sigfig(tbl_medlo$res$RR.crude.wald[[2]])`, `r style_sigfig(tbl_medlo$res$RR.crude.wald[[3]])`, p=`r style_pvalue(tbl_medlo$res$chisq.crude[[3]])`).

</div>

<br>

```{r week4o, warning = FALSE, echo = FALSE, results = 'asis'}

tbl_hbp <-
  lesson4b %>%
  mutate(
    meat_fac =
      as_factor(meat) %>%
      fct_recode("Low" = "1", "Medium" = "2", "High" = "3"),
    hbp_fac =
      as_factor(hbp) %>%
      fct_recode("No Hypertension" = "0", "Hypertension" = "1")
  ) %>%
  select(meat_fac, hbp_fac) %>%
  tbl_summary(
    by = "hbp_fac",
    label = list(meat_fac = "Meat Consumption"),
    row_percent = TRUE)

tbl_hbp %>%
  add_overall(last = TRUE)

```

<br>

You might note that I chose to compare low and medium meat consumption, that is, give all three pairwise comparisons rather than just high v. low and high v. medium. This is probably the exception rather than the rule: in this particular case, it seems worth reporting because the low and medium groups seem very comparable compared to high meat consumption.  

And now, the key point...

You may have noticed that my model answer did not actually answer the question. The question was "does meat consumption lead to hypertension?". I only answered in terms of rates of hypertension rising with increased meat consumption and the like. This is because you cannot use statistics to prove causality without thinking about the research design. If you go out and ask how much meat people eat and then measure their blood pressure, you cannot assume that any associations are causal. It may be, for example, that meat eaters tend to smoke more and it is smoking, not meat, that causes hypertension.

### lesson4c.rds

**This is a data set from a chemotherapy study. The researchers think that a mutation of a certain gene may be associated with chemotherapy toxicity. Should clinicians test for the gene during pre-chemotherapy work up?**

It is immediately obvious from this table that there are very few patients who are homozygous wild type.

```{r week4p, warning = FALSE}

lesson4c %>%
  tabyl(gene, toxicity) %>%
  adorn_title()

```

It would seem appropriate to combine heterozygotes and homozygous wild type into a single category. You might be tempted to type `lesson4c %>% mutate(gene = case_when(gene == 2 ~ 1, TRUE ~ gene))`. But you should avoid changing raw data in this way: it would be preferable to create a new variable as follows:

```{r week4q, warning = FALSE}

lesson4c <-
  lesson4c %>%
  mutate(
    mutant =
      case_when(
        gene %in% c(1, 2) ~ 1,
        gene == 0 ~ 0
      )
  )

```

Moreover, it would be worth reporting the results of an analysis using all three groups, to investigate whether the conclusions change. This is sometimes called a "sensitivity analysis".

```{r week4r, warning = FALSE, include = FALSE}

tbl_gene <-
  tbl_summary(lesson4c %>% select(gene))

tbl_mutant <-
  lesson4c %>%
  tabyl(mutant, toxicity) %>%
  adorn_percentages() %>%
  adorn_pct_formatting(digits = 0)

tbl_mutant_epi <- 
  epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4c, mutant, toxicity, show_na = FALSE), desc(mutant))), 1:2)))

tbl_gene_fisher <-
  lesson4c %>%
  tabyl(gene, toxicity) %>%
  fisher.test()

```

The results of the study are shown in the table. Very few participants (`r tbl_gene$table_body$stat_0[[4]]`) were homozygous mutant. For the purposes of analysis therefore, data for these participants was combined with those heterozygous for the gene. Patients with any mutant allele (i.e. heterozygous or homozygous mutant) had lower rates of toxicity (`r tbl_mutant[2, 3]` vs `r tbl_mutant[1, 3]`, p=`r style_pvalue(tbl_mutant_epi$res$chisq.crude[[3]])` by chi squared). The relative risk of toxicity is `r style_sigfig(tbl_mutant_epi$res$RR.crude.wald[[1]])` compared to patients with the wild-type (95% CI `r style_ratio(tbl_mutant_epi$res$RR.crude.wald[[2]])`, `r style_ratio(tbl_mutant_epi$res$RR.crude.wald[[3]])`). This conclusion is not sensitive to the method of analysis: analysis of the table keeping participants in three groups gives p=`r style_pvalue(tbl_gene_fisher$p.value)` by an exact test. These findings should be confirmed in a larger series. In particular, it would be interesting to know whether patients who are homozygous mutant are at particularly increased risk.

<br>

```{r week4s, warning = FALSE, echo = FALSE, results = 'asis'}

tbl_gene_toxic <-
  lesson4c %>%
  mutate(
    gene_fac =
      as_factor(gene) %>%
      fct_recode("Homozygous wild type" = "0", "Heterozygeous" = "1",
                 "Homozygous mutant" = "2"),
    toxicity_fac =
      as_factor(toxicity) %>%
      fct_recode("No Grade III/IV Toxicity" = "0",
                 "Grade III/IV Toxicity" = "1")
  ) %>%
  select(gene_fac, toxicity_fac) %>%
  tbl_summary(
    by = "toxicity_fac",
    label = list(gene_fac = "Gene"),
    row_percent = TRUE)

tbl_gene_toxic %>%
  add_overall(last = TRUE)

```

<br>

Genetics type people: note that this gene is in Hardy Weinberg equilibrium!

One thing to note is my conclusion, the answer to the question. It would be a little premature to start changing clinical practice on the basis of a single study with a moderate number of patients and a less than overwhelming strength of evidence. Also, it is questionable whether you should start using a test just because it is predictive: you should use a test if it improves clinical outcome.

### lesson4d.rds

**Patients are given chemotherapy regimen a or b and assessed for tumor response. Which regimen would you recommend to a patient? Do the treatments work differently depending on age or sex?**

The first thing we want to do is compare response by group. But using `epi.2by2` with `response` and `chemo` doesn't work. This is because the exposure variable has to be numeric (for example, 0 and 1), not "a" or "b". So you need to use the `group` variable instead of the `chemo` variable.

```{r week4t, include = FALSE, warning = FALSE}

tbl_chemoresponse <-
  lesson4d %>%
  tabyl(group, response) %>%
  adorn_percentages() %>%
  adorn_pct_formatting(digits = 0)

tbl_chemoresponse_epi <- 
  epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4d, group, response, show_na = FALSE), desc(group))), 1:2)))

median_age <-
  style_sigfig(quantile(lesson4d$age, c(0.5), na.rm = TRUE)[[1]])

```

This gives response rates of `r tbl_chemoresponse[1, 3]` on regimen "a" and `r tbl_chemoresponse[2, 3]` on regimen "b", a relative risk of `r style_sigfig(tbl_chemoresponse_epi$res$RR.crude.wald[[1]])` and a pvalue of `r style_pvalue(tbl_chemoresponse_epi$res$chisq.crude[[3]])`. Should we conclude that there is no difference between groups and that you should feel free to use either regimen? Imagine you are a patient. You are told that you have a 50:50 chance of response on one regimen but only a 40% chance on the other. Unless there are good reasons to choose between the regimes (e.g. toxicity), which would you choose? The point here is that statistical significance does not really inform choices between similar alternatives. Assuming that toxicity, cost and inconvenience are similar between the regimes, I would recommend regimen a, even though the difference is not statistically significant. The confidence interval here is absolutely critical: the 95% C.I. for the risk difference is `r style_sigfig( tbl_chemoresponse_epi$res$ARisk.crude.wald[[2]])`%, `r style_sigfig(tbl_chemoresponse_epi$res$ARisk.crude.wald[[3]])`%. In other words, the response rate could be `r style_sigfig(abs( tbl_chemoresponse_epi$res$ARisk.crude.wald[[2]]))`% higher on regimen a (e.g. 55% vs. `r style_sigfig(55 + tbl_chemoresponse_epi$res$ARisk.crude.wald[[2]])`%) or 2% lower (e.g. 50% vs. `r style_sigfig(50 + tbl_chemoresponse_epi$res$ARisk.crude.wald[[3]])`%). So you might do a lot better with regimen a, you are unlikely to do much worse. 

**The sub-group analyses**

A common mistake would be to use `epi.2by2` with `response` and `sex`. However, this would examine whether, regardless of chemotherapy regimen used, men and women had different tumor outcome. What we want to see if the difference between treatment a and b changes depending on whether male or female patients are being treated. As a start, try:

```{r week4u}

# Note the addition of the "filter" function to subset on sex

# Males (sex == 0)
epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4d %>% filter(sex == 0), group, response, show_na = FALSE), desc(group))), 1:2)))

# Females (sex == 1)
epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4d %>% filter(sex == 1), group, response, show_na = FALSE), desc(group))), 1:2)))

```

What you get is a statistically significant difference between treatment groups for men (many more respond on regimen a) but not women (actually a few more respond on regimen b). How to interpret this? In the literature, this might well be trumpeted as showing that though either regimen could be used for women, regimen a is better for men. My own view is that sub-group analyses are hypothesis generating not hypothesis testing. In particular, is there any reason to believe that response to chemotherapy might depend on sex? I don't think there are any cases of this. 

Testing age is a little more difficult as it is a continuous variable. One typical approach might be to create two sub-groups depending on the median. First, `quantile(lesson4d$age, c(0.5), na.rm = TRUE)` gives a median age of `r median_age`. So:

```{r week4v}

# Age > 42
epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4d %>% filter(age > 42), group, response, show_na = FALSE), desc(group))), 1:2)))

# Age <= 42
epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4d %>% filter(age <= 42), group, response, show_na = FALSE), desc(group))), 1:2)))

```

You get no difference between groups for either older or younger patients. As it turns out, this is not a very efficient method of testing for age differences. A better technique will be discussed in the class on regression.

**Interaction or sub-group analysis?**

There is a further problem here: we have one question ("do effects differ between men and women?") and two p values (one for men and one for women). What you really want is one p value to answer one question. The correct statistical technique for looking at whether the effects of treatment differ between sub-groups is called interaction. We will look at this later in the course. In the meantime, if you are interested, you can read some brief articles at: 

http://bmj.bmjjournals.com/cgi/content/full/313/7055/486?ijkey=f0d0304067151ab3c6490b209ca3e954acb60a29&keytype2=tf_ipsecsha

http://bmj.bmjjournals.com/cgi/content/full/313/7060/808?ijkey=7172a78ea07bf92702ec4a33c6804b3ea439c46c&keytype2=tf_ipsecsha

http://bmj.bmjjournals.com/cgi/content/full/313/7061/862?ijkey=c1181f258aa1146139604ac7d463ec93caa4d623&keytype2=tf_ipsecsha

### lesson4e.rds

**This is a lab study of two candidate tumor-suppressor genes (gene1 and gene2). Wild-type mice are compared with mice that have gene1 knocked-out, gene2 knocked-out or both. The presence of tumors is measured after 30 days. Do the genes suppress cancer?**

The obvious thing to do would be to use `epi.2by2` with `cancer` and `gene1`, and then with `cancer` and `gene2`. Superficially this would suggest that gene1 is a tumor suppressor gene and gene2 is not. However, that might be to get too fixed on p-values. The proportion of cancer in mice with gene2 knocked out is over twice that of controls. Remember that animal experiments tend to use a very small number of observations (a typical clinical trial has hundreds of patients, a typical lab study has maybe a dozen mice). So you might want to say that there is good evidence that gene1 is a tumor suppressor gene but that whilst evidence is suggestive for gene2, more research is needed.

A BIG HOWEVER - have a look at the data or this table:

```{r week4w, warning = FALSE}

lesson4e %>%
  tabyl(gene1, gene2) %>%
  adorn_title()

```

```{r week4x, warning = FALSE, include = FALSE}

tbl_gene0 <-
  epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4e %>% filter(gene2==0), gene1, cancer, show_na = FALSE), desc(gene1))), 1:2)))

tbl_gene1 <-
  epi.2by2(as.matrix(select(rev(arrange(tabyl(lesson4e %>% filter(gene2==1), gene1, cancer, show_na = FALSE), desc(gene1))), 1:2)))

```

The experimental design was to knockout no genes in some animals, gene1 in some animals, gene2 in other animals and both genes in a final set of animals. This is known as a factorial or Latin-square design. Analyzing rates of cancer for mice without gene1 or gene2, as suggested above, ignores this experimental design. Presumably the design was implemented because the researchers wanted to know if the effects of gene1 differed depending on whether gene2 had been knocked-out and vice versa. Now you could start doing separate sub-groups (e.g. using `epi.2by2` for `cancer` and `gene1`, filtering by `gene2==0` and `gene2==1` separately) but this produces somewhat unhelpful results. For example, it suggests that gene1 has an effect for wild-type gene2 (p=`r style_pvalue(tbl_gene0$res$chisq.crude[[3]])`) but maybe not for knocked out gene2 (p=`r style_pvalue(tbl_gene1$res$chisq.crude[[3]])`). This has all the look and feel of a spurious sub-group analysis, however. We will discuss some regression approaches to this problem later in the lecture series.

## Week 5

### lesson5a.rds

**These are data from marathon runners (again). Which of the following is associated with how fast runners complete the marathon: age, sex, training miles, weight?**

My guess here is that we are going to want to do a regression analysis. This would allow us to quantify the association between the various predictor variables and race time. For example, it would be interesting to know not only that number of weekly training miles is correlated with race time, but by how much you can reduce your race time if you increased your training by a certain amount. Our dependent variable is "rt" (race time in minutes), so we could type:

```{r week5a}

rt_model <- lm(rt ~ age + sex + tr + wt, data = lesson5a)
summary(rt_model)

```

```{r week5b, echo = FALSE}

rt_N <- nrow(lesson5a)

rt_Nfull <- nobs(rt_model)

rt_tbl1 <-
  lesson5a %>%
  tbl_summary(
    label = list(age = "Age (years)", sex = "Female", tr = "Training miles per week",
                 wt = "Weight (kg)", rt = "Race time (minutes)"),
    statistic = list(..continuous.. = "{mean} ({sd})")
  )

rt_tbl2 <-
  rt_model %>%
  tbl_regression(
    label = list(age = "Age (years)", sex = "Female", tr = "Training miles per week",
                 wt = "Weight (kg)")
  )

male_avg <-
  lesson5a %>%
  filter(sex == 0) %>%
  summarize(mean = mean(rt, na.rm = TRUE)) %>%
  pull(1)

```

The p values for all the predictor variables apart from weight are very low. A model answer might be:

<div class="quote-container">

>There were `r rt_N` runners in the data set; full data were available for `r rt_Nfull`. Summary data for these `r rt_Nfull` are given in table 1. Predictors of race time are shown in table 2. Age, sex and number of training miles were all statistically significant predictors of race time; weight is unlikely to have an important effect.

</div>

\center **Table 1** \center

```{r week5c, echo = FALSE, results = "asis"}

rt_tbl1

```

<br>

\center **Table 2** \center

```{r week5d, echo = FALSE, results = "asis"}

rt_tbl2

```

<br>

A real-life decision you could take away from this is that every additional mile you run in training would be predicted to cut about 1.5 minutes from your marathon time: add a couple of extra five mile runs per week and you’ll shave a quarter of an hour off your time. 

A couple of thoughts. First, you will note that I didn’t remove weight from the model (that is, use a step wise approach) then report the coefficients and p values for the new model (by typing `lm(rt ~ age + sex + tr, data = lesson5a)`).

Second, race time is not normally distributed and the textbooks would have us believe that this invalidates a regression analysis. Let’s try:

```{r week5e}

lesson5a <-
  lesson5a %>%
  mutate(
    lt = log(rt)
  )

rtlog_model <- lm(lt ~ age + sex + tr + wt, data = lesson5a)
summary(rtlog_model)

```

This creates a new variable called "lt" that is the log of race time. The Shapiro-Wilk test confirms that this is normally distributed. Now let’s look at the results of the regression analysis: the p values are almost exactly the same. This suggests that the non-normality of race time is not important in this setting.

<br>

_For advanced students only_

It might be interesting to compare the coefficients from the untransformed and log transformed models. For example, women are predicted to run about `r inline_text(rt_tbl2, variable = "sex", pattern = "{estimate}")` minutes slower on the untransformed model; the coefficient in the log model is `r style_sigfig(rtlog_model$coefficients[[3]], digits = 3)`. Backtransform by typing ``exp(`r style_sigfig(rtlog_model$coefficients[[3]], digits = 3)`)`` and you get `r style_sigfig(exp(rtlog_model$coefficients[[3]]), digits = 3)`. Remember that backtransforming gives you a proportion. So to work out the difference between men and women in race time, first work out the average men’s time:

```{r week5f}

# Get the average time for men
lesson5a %>%
  filter(sex == 0) %>% # Select only males
  skim(rt)

```

Then multiply the men's average (`r style_sigfig(male_avg, digits = 4)`) by `r style_sigfig(exp(rtlog_model$coefficients[[3]]), digits = 3)` to get the women's time, which gives `r style_sigfig(male_avg*exp(rtlog_model$coefficients[[3]]), digits = 4)`.

Subtract one from the other to get `r style_sigfig(male_avg*exp(rtlog_model$coefficients[[3]]) - male_avg, digits = 3)` minutes difference in race time between the sexes. This is very close to the value from the untransformed model.

### lesson5b.rds

**These are data on patients with a disease that predisposes them to cancer. The disease causes precancerous lesions that can be surgically removed. A group of recently removed lesions are analyzed for a specific mutation. Does how long a patient has had the disease affect the chance that a new lesion will have a mutation?**

What you are trying to predict here is binary (mutation / no mutation). You are predicting with a continuous variable (length of disease). So a logistic regression would work nicely. Try:

```{r week5g, results = "asis"}

# Create model
mutate_model <- glm(mutation ~ c, data = lesson5b, family = "binomial")

# Results with odds ratios
tbl_regression(mutate_model, exponentiate = TRUE)

```

<br>

If you get a p-value of around 0.7, you haven't been inspecting your data. One patient has apparently had the disease for 154 years. Either this is a freak of nature or the study assistant meant to type 14 or 15. You need to delete this observation from the dataset and try the model again:

```{r week5h, results = "asis"}

lesson5b <-
  lesson5b %>%
  filter(c <= 50)

# Create model
mutate_model <- glm(mutation ~ c, data = lesson5b, family = "binomial")

# Results in odds ratios
tbl_regression(mutate_model, exponentiate = TRUE)

```

<br>

```{r week5i, results = "asis"}

# Results in logits
tbl_regression(mutate_model, exponentiate = FALSE)

```

<br>

```{r week5j, echo = FALSE}

mutate_or <-
  mutate_model %>%
  tbl_regression(exponentiate = TRUE)

mutate_logit <-
  mutate_model %>%
  tbl_regression(intercept = TRUE,
                 exponentiate = FALSE)

log_calc10 <-
  mutate_logit$table_body$estimate[2]*10 +
  mutate_logit$table_body$estimate[1]

log_exp10 <-
  exp(log_calc10)/(1 + exp(log_calc10))*100

log_calc3 <-
  mutate_logit$table_body$estimate[2]*3 +
  mutate_logit$table_body$estimate[1]

log_exp3 <-
  exp(log_calc3)/(1 + exp(log_calc3))*100

```

After fixing the data, you will now find an association (`r inline_text(mutate_or, variable = "c", pattern = "{p.value}")`). Using the `tbl_regression` function, you get an odds ratio of `r inline_text(mutate_or, variable = "c", pattern = "{estimate}")`, meaning that the odds of having a mutation increase by `r inline_text(mutate_or, variable = "c", pattern = "{estimate}")` each year.

You can get the coefficients in logits from the `tbl_regression` function by using the `exponentiate = FALSE` option.

```{r week5k, results = "asis"}

# Results in logits
# The "intercept = TRUE" option includes the constant in the table
tbl_regression(mutate_model,
               exponentiate = FALSE,
               intercept = TRUE)

```

<br>

This gives a coefficient of `r inline_text(mutate_logit, variable = "c", pattern = "{estimate}")` and a constant of `r inline_text(mutate_logit, variable = "(Intercept)", pattern = "{estimate}")`. This can be used to make a specific prediction using the equation below ("l" stands for the log odds).

$p = e^l / (e^l+1)$

Take someone who has had the disease for 10 years. Their logit is 10 times the coefficient of `r inline_text(mutate_logit, variable = "c", pattern = "{estimate}")` minus the constant `r inline_text(mutate_logit, variable = "(Intercept)", pattern = "{estimate}")` to give `r style_sigfig(log_calc10)`. Plug `r style_sigfig(log_calc10)` into the equation (``exp(`r style_sigfig(log_calc10)`)/(exp(`r style_sigfig(log_calc10)`)+1)``) and you get `r style_sigfig(log_exp10)`%. Someone who has had the disease for 3 years has a logit of `r style_sigfig(log_calc3)` giving a risk (``exp(`r style_sigfig(log_calc3)`)/(exp(`r style_sigfig(log_calc3)`)+1)``) of `r style_sigfig(log_exp3)`%.

If you wanted to get flashy, you could work out the risk for each year between 2 and 18 and get a graph. But R can do this automatically for you: you can create a variable with the risk of mutation for each observation based on the duration of disease using the `augment` function. The column that contains the risk is called ".fitted". Since this is a logistic regression model, we also need to specify `type.predict = "response"` to get the predicted probabilities.

```{r week5l}

# Create predictions
lesson5b_pred <-
  augment(mutate_model,
          newdata = lesson5b,
          type.predict = "response")

# Show resulting dataset
lesson5b_pred

# Graph over disease duration

# Tells the plot what data and variables to use
ggplot(data = lesson5b_pred, aes(x = c, y = .fitted)) +
  # Creates the line portion of the graph
  geom_line() +
  # Creates the points along the line
  geom_point() +
  # Labels the x and y axes
  labs(
    x = "Disease duration",
    y = "Risk of mutation"
  )

```

<br>

You can also use the `augment` function for linear regression and multivariable models. One other interesting thing you can do is to make "out of sample" predictions.

Try this:

```{r week5m, results = "asis"}

# Reopen lesson5b dataset
lesson5b <- readRDS(here::here("Data", "Week 5", "lesson5b.rds"))

# Use original logistic regression model
tbl_regression(mutate_model, exponentiate = TRUE)

```

<br>

```{r week5n, results = "asis"}

# Create new values for "c" (disease duration)
lesson5b_new <-
  lesson5b %>%
  mutate(c = 1:n()/3)
# "1:n()" gives the observation number for each observation
# Replace c with observation number / 3 gives a list of simulated
# disease durations between 0.33 and 42.33.

# Predict risk of mutation
lesson5b_pred <-
  augment(mutate_model,
          newdata = lesson5b_new,
          type.predict = "response")

# Create graph
ggplot(data = lesson5b_pred,
       aes(x = c, y = .fitted)) +
  geom_line()

```

### lesson5c.rds

**These are data from Canadian provinces giving population, unemployment rates and male and female life expectancy. Which of these variables are associated?**

A key first question: what are we likely to be interested in? For example, total population and unemployment rate will have no illuminating relationship whatsoever and we wouldn’t want to analyze the association between these two. The link between unemployment and life expectancy seems more interesting. Before we start doing the analysis however, we need to look at the data.

```{r week5o}

# Open up dataset to view observations
View(lesson5c)

```

It seems that there are not only data for each province separately but for Canada as a whole. Clearly we would have to delete this row as it is not an independent observation.

```{r week5p}

lesson5c_fixed <-
  lesson5c %>%
  filter(place != "Canada")

```

Now, we could just correlate the three variables together: 

```{r week5q}

# There are missing values for unemployment, so we need to indicate that we only want to use "complete observations"
cor(lesson5c_fixed %>% select(unemp, mlife, flife),
    use = "complete.obs")

```

This tells you that unemployment rates are negatively correlated with life expectancy (i.e. the higher the unemployment rate, the lower the life expectancy), that this effect seems stronger for men than for women and that male and female life expectancy are strongly correlated.

We might also want to go on and do some regressions. We probably wouldn’t ever want to predict the unemployment rate on the basis of life expectancy, but the opposite case is indeed of interest: we might want to know, for example, what effect a 1% drop in unemployment rate might have on life expectancy. 

Try these regression models:

```{r week5r, results = "asis"}

mlife_model <- lm(mlife ~ unemp, data = lesson5c_fixed)
tbl_regression(mlife_model)

```

<br>

```{r week5s, results = "asis"}

flife_model <- lm(flife ~ unemp, data = lesson5c_fixed)
tbl_regression(flife_model)

```

<br>

The coefficients you get (`r inline_text(tbl_regression(mlife_model), variable = "unemp", pattern = "{estimate}")` for men and `r inline_text(tbl_regression(flife_model), variable = "unemp", pattern = "{estimate}")` for women) suggest that a 5% drop in unemployment is associated with about a 6 month increase in life expectancy, in other words, a small effect.

Interesting question: should you also report a p value and 95% confidence interval for the coefficients? The answer is no. This is because Canadian provinces are not some random sample from a large theoretical population of Canadian provinces. You have all the data. So questions of inference (i.e. p values) and uncertainty (i.e. 95% confidence intervals) don’t come into it.

Even more interesting question: the conclusion of a 5% drop in unemployment being associated with a 6 month increase in life expectancy is based on a causal relationship, that is, we believe that changes in unemployment _cause_ changes in life expectancy. This is not implausible: unemployment leads to poverty and depression, both of which reduce life expectancy. However, a causal relationship cannot be assumed. For example, it may be that life expectancy is lower in certain areas of the country (due to eating patterns or ethnic differences) that also happen to suffer higher unemployment. 

### lesson5d.rds

**These are data from mice inoculated with tumor cells and then treated with different doses of a drug. The growth rate of each animal’s tumor is then calculated. Is this drug effective?**

This is a straightforward linear regression that shows a significant decrease in log tumor size with increasing dose.

```{r week5t}

tumorsize_model <- lm(s ~ dose, data = lesson5d)
summary(tumorsize_model)

```

A couple of thoughts. Firstly, this rather obvious analysis is not often conducted in lab research. Typically, researchers present pairwise comparisons between each dose and control. For example, see the following typical diagram. Each of the p values compares the dose to control. The problem with such an approach is that you end up with multiple p values (instead of just one) and that each test takes place in a vacuum: the p value of a comparison between no treatment and, say, dose level 3, is the same regardless of whether there were 2 dose levels in the experiment, or 100, and whether all other dose levels showed an effect or did not. A regression analysis gives you a single p value and uses all data in one analysis.

```{r week5u, echo = FALSE, results = "asis", fig.width = 4, fig.height = 4}

# Create dataset to create figure
df_graph <-
  tribble(
    ~x, ~y, ~p, ~sd,
    0, 100, "", 8,
    25, 110, "p=0.219", 10,
    50, 115, "p=0.0033", 15,
    100, 120, "p=0.0001", 12
  ) %>%
  mutate(x = factor(x))

ggexample <-
  ggplot(data = df_graph) +
  geom_bar(aes(x = x, y = y),
           stat = "identity",
           width = 0.5,
           fill = "gray80",
           color = "black",
           position = "dodge") +
  geom_errorbar(aes(ymin = y, ymax = y + sd, x = x),
                width = 0.2) +
  geom_text(aes(x = x, y = y, label = p),
            position = position_dodge(width = 0.9),
            vjust = -4.25) +
  scale_y_continuous(limits = c(0, 140), breaks = seq(0, 140, by = 20)) +
  labs(
    x = "",
    y = "CFU (% of Control)"
  )

plot(ggexample)

```

A second thought: should you report the coefficient of `r tumorsize_model$coefficients[[2]] %>% style_sigfig(digits = 5)`? This can be interpreted as "for each increase in dose of one unit, increase in log tumor size is less by `r tumorsize_model$coefficients[[2]] %>% style_sigfig(digits = 5)`". I would argue that this coefficient might misleading because you only have a few doses and they are widely spaced. Also, dose-response is often non-linear: it generally takes a sigmoidal curve (see below). In short, you might have too few data to be confident about a linear prediction. Finally, it is rare that you really want to estimate the effects of a treatment on a mouse: generally, laboratory studies are about testing hypotheses.

```{r week5v, echo = FALSE, results = "asis", fig.width = 4, fig.height = 4}

# Dose response curve

sigmoid = function(x) {
  1 / (1 + exp(-x))
}

df_graph2 <-
  tibble(
    x = seq(-5, 5, by = 0.1),
    y = sigmoid(x)
  )

ggexample2 <-
  ggplot(data = df_graph2,
         aes(x = x, y = y)) +
  geom_line() +
  labs(
    x = "Dose",
    y = "Response"
  ) +
  scale_y_continuous(breaks = NULL) +
  scale_x_continuous(breaks = NULL) +
  theme_bw()

plot(ggexample2)

```

My model answer would be:

<div class="quote-container">

>Mean increase in log tumor size per day by dose is given in the table. Higher doses were associated with lower growth rates (`r inline_text(tbl_regression(tumorsize_model), variable = "dose", pattern = "{p.value}")` by linear regression).

</div>

```{r week5w, echo = FALSE, results = "asis"}

df_tbl5d <-
  tbl_summary(
    lesson5d %>% select(dose, s),
    by = "dose",
    statistic = list(..continuous.. = "{mean} ({sd})"))

df_tbl5dcol <-
  df_tbl5d$table_body %>%
  gather(v, value, -variable, -row_type, -label) %>%
  bind_cols(
    lesson5d %>% select(dose) %>% unique()
  ) %>%
  select(dose, value) %>%
  gt() %>%
  cols_label(dose = "Dose",
             value = "Mean (SD)")

df_tbl5dcol

```

<br>

### lesson5e.rds

**These are data from a study of the use of complementary medicine (e.g. massage) by UK breast cancer patients. There are data for the women’s age, time since diagnosis, presence of distant metastases, use of complementary medicine before diagnosis, whether they received a qualification after high school, the age they left education, whether usual employment is a manual trade, socioeconomic status. What predicts use of complementary medicine by women with breast cancer?**

This is a fairly simple analysis: the outcome is binary and as we want to make predictions we are going to want a logistic regression. It would be tempting just to throw all the variables into the analysis:

```{r week5x}

cam_model1 <- glm(CAM ~ age + t + mets + u + q18 + e + m + ses,
                  data = lesson5e,
                  family = "binomial")
tbl_regression(cam_model1, exponentiate = TRUE)

```

<br>

This would suggest that the strongest predictor is whether women used complementary medicine before diagnosis and that two other variables are predictive: women with a qualification after leaving high school are more likely to use complementary medicine, but use declines with age (older people are more conservative, or alternatively, wiser, depending on your point of view.)

However...

Should we just throw everything in the model? For example, only 6 patients have distant metastases. Secondly, many of the variables seem highly correlated. For example, age at which the patient left education and whether they received a qualification after the age of 18 are highly correlated as are socioeconomic status and whether the patient’s job is a manual trade. In clinical terms, you wouldn’t need to know the age that someone left education if you already knew whether they had a post-high school qualification. And as would be intuitive, regression does not deal well with correlated variables. To take an extreme example, take two variables "x" which is equal to height, and "y" which is equal to height plus 1 inch. These have a correlation of one. Regressing just variable x or y on shoe size you get:

$shoesize = 0.6*x - 30$

or

$shoesize = 0.6*y - 31$

What would you get for regressing x and y? Any of the following are possible:

$shoesize = 0.3*x + 0.3*y - 30.5$

$shoesize = 0.4*x + 0.2*y - 30.5$

$shoesize = 0.1*x + 0.5*y - 30.5$

There is no way of telling these apart in terms of regression. Bottom line: care is needed with correlated variables both in a clinical  / scientific sense (you don’t need both of two correlated variables in a model) and a statistical sense (regression results can be misleading).

So I would start with the full model and then remove the least predictive of socioeconomic status and manual trade, and the least predictive of qualification and age left education. You end up with:

```{r week5y}

cam_model2 <- glm(CAM ~ age + t + mets + u + q18 + ses,
                  data = lesson5e,
                  family = "binomial")
tbl_regression(cam_model2, exponentiate = TRUE)

```

<br>

Now it appears that "q18" is predictive but not "ses". So try removing one of these from the model in turn:

```{r week5z, results = "asis"}

cam_model3 <- glm(CAM ~ age + t + mets + u + ses,
                  data = lesson5e,
                  family = "binomial")
tbl_regression(cam_model3, exponentiate = TRUE)

```

<br>

```{r week51, results = "asis"}

cam_model4 <- glm(CAM ~ age + t + mets + u + q18,
                  data = lesson5e,
                  family = "binomial")
tbl_regression(cam_model4, exponentiate = TRUE)

```

<br>

It turns out that both are predictive independently, but not together. When you think about this, it is because socioeconomic status is correlated with education. The most sensible thing to do is to leave "q18" in the model as a) it is slightly more predictive and b) it is easier to ask about; c) there are fewer missing data. As such, my model answer might be:

<div class="quote-container">

>There were responses from `r nrow(lesson5e)` women, of whom `r nrow(cam_model4)` had data adequate for analysis. There were more missing data for socioeconomic status but this was not included in the final model. In a multivariable model, use of complementary medicine before diagnosis was the strongest predictor of complementary medicine use in women with breast cancer, odds ratio `r inline_text(tbl_regression(cam_model4, exponentiate = TRUE), variable = "u")`. Complementary medicine use was related to education. The odds of a woman with a qualification received after the age of 18 using complementary medicine were `r inline_text(tbl_regression(cam_model4, exponentiate = TRUE), variable = "q18")` those of women without a qualification after high school. In a univariate model, higher socioeconomic status predicted use of complementary medicine, but this was dropped from the multivariable model due to collinearity with educational achievement. Older women were less likely to use complementary medicine `r inline_text(tbl_regression(cam_model4, exponentiate = TRUE), variable = "age", pattern = "(odds ratio {estimate} for a one-year increase in age; {conf.level*100}% CI {conf.low}, {conf.high}, {p.value})")`. There was no statistically significant effect of time since diagnosis `r inline_text(tbl_regression(cam_model4, exponentiate = TRUE), variable = "t", pattern = "(odds ratio {estimate} per year; {conf.level*100}% CI {conf.low}, {conf.high}, {p.value})")`. This suggests that breast cancer patients who use complementary medicine are likely to start doing so shortly after diagnosis.

</div>

### lesson5f.rds

**These are the distance records for Frisbee for various ages in males. What is the relationship between age and how far a man can throw a Frisbee?**

Well the obvious thing to do would be a linear regression:

```{r week52}

frisbee_model <- lm(distance ~ age,
                    data = lesson5f)
summary(frisbee_model)

```

You get a coefficient of `r style_sigfig(frisbee_model$coefficients[[2]], digits = 3)`, meaning that the furthest a man can throw a Frisbee increases by `r style_sigfig(frisbee_model$coefficients[[2]], digits = 3)` meters every year. But of course this is nonsense: you can’t throw further and further each year, athletic ability starts to decline with age. You can see this on a graph:

```{r week53}

ggplot(data = lesson5f,
       aes(x = age, y = distance)) + 
  geom_point()

```

<br>

In other words, there is not a linear relationship between your age and the distance you can throw a Frisbee. There are two options. You can either do two linear regressions, one for age up to 30 and one for age >30. But this is a little sloppy: better, you could try including non-linear terms (see below). In either case, remember not to report p values or 95% confidence intervals: these are meaningless because there is only one record per distance.

<br>

_For advanced students only:_

Often the relationship between an $x$ and a $y$ does not follow a straight line. You may remember from high school that one way to get a curve is to have a quadratic equation including both $x$ and $x^2$. So you have to create a new variable called age2 as "age2 = age^2" and regress:

```{r week54}

lesson5f <-
  lesson5f %>%
  mutate(
    age2 = age^2
  )

frisbee_model2 <- lm(distance ~ age + age2,
                     data = lesson5f)
summary(frisbee_model2)

```

```{r week55, echo = FALSE}

agecoef <-
  style_sigfig(frisbee_model2$coefficients[[2]], digits = 3)

age2coef <-
  style_sigfig(frisbee_model2$coefficients[[3]], digits = 3)

```

This model (distance = 11.6*age - 0.149*age^2 + 8.13) fits extremely well (r^2^ of `r style_sigfig(summary(frisbee_model2)$r.squared)` vs `r style_sigfig(summary(frisbee_model)$r.squared)` for the linear model). If you really remember your high school match, consider that for a regression equation $y = ax + bx^2 + c$ you can work out the maximum $y$ as $-a/2b$. We predict that the world record for distance will be held by a man who is -`r agecoef`/(2 * `r age2coef`) = `r style_sigfig(-frisbee_model2$coefficients[[2]]/(2 * frisbee_model2$coefficients[[3]]), digits = 3)` years old.

### lesson5g.rds

**You’ve seen this data set before. Patients with lung cancer are randomized to receive either chemotherapy regime a or b and assessed for tumor response. We know there is no difference between regimes (you can test this if you like).  However, do the treatments work differently depending on age or sex?**

The first thing to do here is to do the sub-group analysis, just to get a feel for the data. 

Try this:

```{r week56}

# Putting 3 variables into the "tabyl" function gives a two-way table of the first two variables, separately by the third variable
lesson5g %>%
  tabyl(response, chemo, sex) %>%
  adorn_percentages("col") %>%
  adorn_pct_formatting() %>%
  adorn_title()

# Here, summarize stores out the p values for the chi-squared test by sex
lesson5g %>%
  group_by(sex) %>%
  summarize(p = chisq.test(response, chemo, correct = FALSE)$p.value)

```

```{r week57, echo = FALSE, warning = FALSE}

chitable1 <-
  lesson5g %>%
  tabyl(response, chemo, sex) %>%
  adorn_percentages("col") %>%
  adorn_pct_formatting(digits = 0)

chip1 <-
  lesson5g %>%
  group_by(sex) %>%
  summarize(p = chisq.test(response, chemo, correct = FALSE)$p.value)

# chitable2 <-
#   lesson5g %>%
#   tabyl(response, sex) %>%
#   adorn_percentages("col")%>%
#   adorn_pct_formatting()

```

This shows that regime a is better for men (`r chitable1[[1]][2, 3]` vs `r chitable1[[1]][2, 2]` response rate, p=`r style_pvalue(chip1[1, 2])`). For women, there doesn't seem to be a difference between groups. The next thing to check is whether sex in general makes a difference to response:

```{r week58, warning = FALSE}

lesson5g %>%
  tabyl(response, sex) %>%
  adorn_percentages("col")%>%
  adorn_pct_formatting() %>%
  adorn_title()

lesson5g %>%
  tabyl(response, sex) %>%
  chisq.test(correct = FALSE)

```

This shows similar response rates (around 40-45%) for men and women.

Now we want to test for interaction between chemo regime and sex in a multivariable regression. You can put the interaction directly into the multivariable model:

```{r week59}

sex_int_model <- glm(response ~ sex + chemo + sex*chemo,
                     data = lesson5g,
                     family = "binomial")
tbl_regression(sex_int_model, exponentiate = TRUE)

```

<br>

By the way, don't try `glm(response ~ sex*chemo, ...)`, that is, including only the interaction term. This would involve a non-randomized comparison between men and women who received the same chemotherapy treatment.

You can repeat this for age by creating a new variable based on the median age:

```{r week510}

# First, get the median age
skim(lesson5g$age)

# Then, create a category variable based on median age
lesson5g <-
  lesson5g %>%
  mutate(
    hiage = if_else(age > 42.5, 1, 0)
  )

```

Then do the subgroup analysis as above:

```{r week511}

lesson5g %>%
  filter(!is.na(age)) %>% # Exclude 2 patients missing age
  tabyl(response, chemo, hiage) %>%
  adorn_percentages("col")%>%
  adorn_pct_formatting() %>%
  adorn_title()

lesson5g %>%
  filter(!is.na(age)) %>% # Exclude 2 patients missing age
  group_by(hiage) %>%
  summarize(p = chisq.test(response, chemo, correct = FALSE)$p.value)

```

There do not appear to be any differences between groups. Now to do the interaction analysis, you can create an interaction term in one of two ways (`chemo*age` or `chemo*hiage`).

```{r week512, results = "asis"}

age_int_model1 <-
  glm(response ~ chemo + age + chemo*age,
      data = lesson5g,
      family = "binomial")
tbl_regression(age_int_model1, exponentiate = TRUE)

```

<br>

```{r week513, results = "asis"}

age_int_model2 <-
  glm(response ~ chemo + hiage + chemo*hiage,
      data = lesson5g,
      family = "binomial")
tbl_regression(age_int_model2, exponentiate = TRUE)

```

<br>

```{r week514, echo = FALSE}

chitable2 <-
  lesson5g %>%
  tabyl(response, chemo, hiage) %>%
  adorn_percentages("col") %>%
  adorn_pct_formatting(digits = 0)

```

Whichever one you choose, the interaction is non-significant in a logistic regression with age and chemotherapy. Incidentally, you must put age in the model because, not surprisingly, it is statistically significant, with higher age being associated with a lower chance of response.

Incidentally, I have often emphasized the importance of including an estimate and a 95% confidence interval in any set of results rather than just saying "differences between groups were / were not significant". Interaction analyses are somewhat of an exception to this general rule. First, these tend to be secondary, exploratory analyses. Secondly, interaction is rather rare in medicine: treatments tend to work (or not work) regardless of a patients age, gender, race and so on. Thirdly, the coefficient for the interaction term can be quite difficult to interpret. So it is often appropriate just to say that you looked for an interaction and didn't find one. A model answer might be:

<div class="quote-container">

>[Describe main comparison between chemo regimens here. Describe age and sex here.] In a subgroup analysis, there appeared to a differential effect of the two regimens depending on sex. In men, response rates were higher for regimen B (`r chitable1[[1]][2, 3]`) than regimen A (`r chitable1[[1]][2, 2]`); response rates were more similar in women, though perhaps favoring regimen B (`r chitable1[[2]][2, 3]` vs. `r chitable1[[2]][2, 2]`). Sex and sex by regimen interaction were entered into a logistic regression model of regimen and response, but the interaction term was non-significant (p=`r style_pvalue(tbl_regression(sex_int_model, exponentiate = TRUE)$table_body$p.value[[3]])`). There were no apparent differences in response depending on age: response rates were `r chitable2[[1]][2, 3]` v. `r chitable2[[1]][2, 2]` in patients aged below the median compared to `r chitable2[[2]][2, 3]` v. `r chitable2[[2]][2, 2]` in patients aged above the median. The interaction between regimen and age as a continuous variable was non-significant (p=`r style_pvalue(tbl_regression(age_int_model1, exponentiate = TRUE)$table_body$p.value[[3]])`).

</div>

### lesson5h.rds

**PSA is used to screen for prostate cancer. In this data set, the researchers are looking at various forms of PSA (e.g. "nicked" PSA). What variables should be used to try to predict prostate cancer? How accurate would this test be?**

```{r week515, echo = FALSE, message = FALSE}

psa_model <- glm(cancer ~ psa + psan + psai + psant,
                 data = lesson5h,
                 family = "binomial")
tbl_psa_model <- tbl_regression(psa_model, exponentiate = TRUE)

tbl_psa <-
  lesson5h %>%
  select(-cancer) %>%
  tbl_summary(
    label = list(psa = "Total PSA (ng/mL)",
                 psan = "Nicked PSA (ng/mL)",
                 psai = "Intact PSA (ng/mL)",
                 psant = "Nicked-to-total ratio")
  )

pred_psa <-
  augment(
    psa_model, type.predict = "response"
  )

auc_psa <- auc(pred_psa$cancer, pred_psa$.fitted)

nont_model <- glm(cancer ~ psa + psan + psai,
                  data = lesson5h,
                  family = "binomial")

pred_nont <-
  augment(nont_model, type.predict = "response")

auc_nont <- auc(pred_nont$cancer, pred_nont$.fitted)

pred_noi <-
  augment(
    glm(cancer ~ psa + psan,
        data = lesson5h,
        family = "binomial"),
    type.predict = "response"
  )

auc_noi <- auc(pred_noi$cancer, pred_noi$.fitted)

```

One approach to selecting which variables to include in a predictive model would be to do a regression. You could start with all the variables (i.e. `glm(cancer ~ psa + psan + psai + psant, ...)`). You might notice that psant is not a good predictor (p=`r style_pvalue(tbl_psa_model$table_body$p.value[[4]])`) and decide to take it out of the model. In a regression using the remaining three variables, psai is not statistically significant, and you might then want to remove that variable too. Both psa and psan are highly predictive, so you could leave them in the model and use the `roc` function (from the `pROC` package) to get the area-under-the-curve.

However, I would have my doubts about such an approach. What you are trying to do here is predict the presence of cancer as well as you can: the significance or otherwise of individual variables doesn’t really come into it. How I would lay about my analysis would be in terms of the area-under-the-curve of the model. So a model answer might be:

<div class="quote-container">

>The cohort consisted of `r nobs(psa_model)` patients. Values for PSA and PSA subtypes are shown in the table. Entering all four PSA variables into a logistic model to predict the presence of prostate cancer gave an area-under-the-curve (AUC) of `r style_sigfig(auc_psa, digits = 3)`. Nicked-to-total ratio was not a strong predictor and removing it from the model did not decrease AUC (`r style_sigfig(auc_nont, digits = 3)`). Removing intact PSA from the model had a small but noticeable impact on AUC (`r style_sigfig(auc_noi, digits = 3)`). We therefore recommend that further research use total, nicked and intact PSA to predict prostate cancer.

</div>

\center **Table 1. Marker distributions** \center

```{r week516, echo = FALSE, results = "asis"}

tbl_psa

```

<br>

\center **Table 2. Multivariable prediction model** \center

```{r week517, echo = FALSE, results = "asis"}

tbl_regression(nont_model,
               exponentiate = TRUE,
               label = list(psa = "Total PSA",
                            psan = "Nicked PSA",
                            psai = "Intact PSA"))

```

### lesson5i.rds

**This is a randomized trial of behavioral therapy in cancer patients with depressed mood. Patients are randomized to no treatment (group 1), informal contact with a volunteer (group 2) or behavior therapy with a trained therapist (group 3). What would you conclude about the effectiveness of these treatments?**

One approach to these data is to use a test called ANOVA. This addresses the question of whether there is some overall difference between the three groups, or, more specifically, tests the null hypothesis that the three groups are equivalent. But this isn’t a very interesting hypothesis. The other thing to do would be to conduct t tests on all possible pairs (i.e. no treatment vs. therapy; therapy vs. volunteer; volunteer vs. no treatment). But I am not sure that this is particularly interesting either.

What I would use is a regression analysis.

```{r week518}

lesson5i <-
  lesson5i %>%
  mutate(
    treat = if_else(group > 1, 1, 0),
    therapy = if_else(group == 3, 1, 0),
  )

treat_model <- lm(followup ~ baseline + treat + therapy,
                  data = lesson5i)
tbl_regression(treat_model)

```

<br>

What this does is to create two dummy variables "treat" and "therapy". "treat" means that you had some kind of treatment, whether that was contact with the therapist or just with a volunteer. "therapy" means you say the trained therapist. So the groups are coded:

```{r week519, echo = FALSE, results = "asis"}

tbl_5i <-
  tribble(
    ~Group, ~treat, ~therapy,
    "No treatment", 0, 1,
    "Volunteer", 1, 0,
    "Trained therapy", 1, 1
  ) %>%
  gt()

tbl_5i

lesson5i_diff <-
  lesson5i %>%
  mutate(
    diff = followup - baseline
  ) %>%
  select(diff, group) %>%
  group_by(group) %>%
  summarize(mean = mean(diff, na.rm = TRUE),
            sd = sd(diff, na.rm = TRUE))

```

<br>

Now when you regress the change score using the variables "treat" and "therapy" you get an estimate of the effect of just spending time with someone and the effect of seeing a trained professional. In my view, this fits in well with the original study design. A model answer might be:

<div class="quote-container">

> Mood scores improved in the therapist group (`r style_sigfig(lesson5i_diff$mean[[3]], digits = 2)` points, SD `r style_sigfig(lesson5i_diff$sd[[3]], digits = 2)`) and volunteer groups (`r style_sigfig(lesson5i_diff$mean[[2]], digits = 2)`, SD `r style_sigfig(lesson5i_diff$sd[[2]], digits = 2)`) but not in the no treatment group (`r style_sigfig(abs(lesson5i_diff$mean[[1]]), digits = 1)` point worsening in score, `r style_sigfig(lesson5i_diff$sd[[1]], digits = 2)`). Interaction with a considerate individual appears to improve mood by `r inline_text(tbl_regression(treat_model), variable = "treat", pattern = "{estimate} points ({conf.level*100}% CI {conf.low}, {conf.high}, {p.value})")` with active behavior therapy further improving scores an additional `r inline_text(tbl_regression(treat_model), variable = "therapy", pattern = "{estimate} points ({conf.level*100}% CI {conf.low}, {conf.high}, {p.value})")`.

</div>

## Week 6

### lesson6a.rds and lesson6b.rds

**These are data on a blood test (creatine kinase) to detect a recent myocardial infarct. The two data sets are from a coronary care unit population (06b) and a general hospital population (06c). What is the sensitivity, specificity, positive predictive value and negative predictive value?**

```{r week6a, echo = FALSE, warning = FALSE, results = "asis"}

ss_6a <-
  as.matrix(select(rev(arrange(
    tabyl(lesson6a, cktest, mi),
    desc(cktest))), 1:2))

tbl_ss6a <-
  epi.tests(ss_6a)$elements

ss_6b <-
  as.matrix(select(rev(arrange(
    tabyl(lesson6b, cktest, mi),
    desc(cktest))), 1:2))

tbl_ss6b <-
  epi.tests(ss_6b)$elements

tbl_ss <-
  tibble(
    test = c("se", "sp", "ppv", "npv")
  ) %>%
  mutate(
    value_6a =
      round(
        map_dbl(
          test,
          ~ tbl_ss6a %>%
            pluck(..1)
        ) * 100, 0),
    value_6b =
      round(
        map_dbl(
          test,
          ~ tbl_ss6b %>%
            pluck(..1)
        ) * 100, 0)
  ) %>%
  mutate(
    test =
      case_when(
        test == "se" ~ "Sensitivity",
        test == "sp" ~ "Specificity",
        test == "ppv" ~ "Positive Predictive Value",
        test == "npv" ~ "Negative Predictive Value"
      )
  ) %>%
  gt %>%
  cols_label(
    test = "",
    value_6a = "Coronary Care Population",
    value_6b = "General Hospital Population"
  )

```

<br>

The thing to take away from this is that sensitivity and specificity don’t change, but the positive and negative predictive value do. The prevalence of myocardial infarct is obviously much lower in the general hospital population compared to the coronary care population. So negative predictive value is higher in general patients (you are unlikely to have an MI anyway, so if the test says you’re negative, it is almost definite) and positive predictive value higher in coronary care patients (you at high risk of having an MI, so a positive test just about confirms it).

### lesson6c.rds

**Here are the data from a study of a marker to predict the results of biopsy for cancer. There are 2000 patients, half of whom had a suspicious imaging result and were biopsied. It is known that only about half of patients with abnormal imaging actually have cancer and that is what is found in this study. The marker was measured in patients undergoing biopsy. Might the new marker help decide which patients with abnormal scans should receive biopsy?**

You might be tempted just to try:

```{r week6b}

roc(cancer, marker, data = lesson6c)

```

You'd get an AUC of `r style_sigfig(auc(roc(cancer, marker, data = lesson6c)), digits = 3)`, which is pretty good. But the question isn't "how good is the marker?" but "might the new marker help make a clinical decision about biopsy?" So we need to look at clinical consequences.

Let's try:

```{r week6c, warning = FALSE}

lesson6c %>%
  tabyl(cancer, marker) %>%
  adorn_totals() %>%
  adorn_title()

```

So you can see that, if you only biopsied patients who were positive for the marker, you do 600 fewer biopsies per 1000, but you’d also miss 200 cancers. That is a lot of cancers to miss and so you might feel that using the marker to make biopsy decisions would do more harm than good. 

### lesson6d.rds

**This is a data set of cancer patients undergoing surgery with the endpoint of recurrence within 5 years. Since this cohort was established, adjuvant therapy has been shown to be of benefit. Current guidelines are that adjuvant therapy should be considered in patients with stage 3 or high grade disease. Recently, two new variables have been added to the data set:  levels of a novel tumor marker were obtained from banked tissue samples and preoperative imaging scans were retrieved and scored from 0 (no evidence of local extension) to 4 (definite evidence of local extension). Here are some questions about these data:**

**- How good is the current method for determining whether patients should be referred to adjuvant therapy?**
**- It has been suggested that a statistical model using stage and grade would be better than the current risk grouping. How good do you think this model would be?**
**- Does the marker add information to the model of stage and grade?**
**- Does imaging add information to the model including stage, grade and the marker?**

The first question concerns the value of the current method of determining referral for adjuvant therapy. There are two ways of thinking about this. The first is to show a simple table:

```{r week6c}

lesson6d %>%
  tabyl(recurrence, hi_risk)


```
